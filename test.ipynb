{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_si_probs(tags, word_indices, log_forward, log_backward, log_forward_val, transition, emission):\n",
    "    si_probabilities = np.full((len(tags), len(word_indices)-1, len(tags)), -np.inf)\n",
    "\n",
    "    for i in range(len(word_indices)-1):\n",
    "        for j in range(len(tags)):\n",
    "            for k in range(len(tags)):\n",
    "                si_probabilities[j, i, k] = (\n",
    "                    log_forward[j, i] +\n",
    "                    log_backward[k, i+1] +\n",
    "                    transition[j, k] +\n",
    "                    emission[k, word_indices[i+1]] -\n",
    "                    log_forward_val\n",
    "                )\n",
    "    return si_probabilities\n",
    "\n",
    "\n",
    "\n",
    "def log_si_probs_vec(tags, word_indices, log_forward, log_backward, log_forward_val, transition, emission):\n",
    "    \"\"\"\n",
    "    Vectorized implementation of log_si_probs function.\n",
    "    \n",
    "    Parameters:\n",
    "    - tags: List of possible tags\n",
    "    - word_indices: Sequence of word indices\n",
    "    - log_forward: Log forward probabilities\n",
    "    - log_backward: Log backward probabilities\n",
    "    - log_forward_val: Log forward value\n",
    "    - transition: Transition log probabilities matrix\n",
    "    - emission: Emission log probabilities matrix\n",
    "    \n",
    "    Returns:\n",
    "    - si_probabilities: Vectorized log probabilities tensor\n",
    "    \"\"\"\n",
    "    # Determine dimensions\n",
    "    num_tags = len(tags)\n",
    "    num_words = len(word_indices) - 1\n",
    "    \n",
    "    # Create the output tensor filled with -inf\n",
    "    si_probabilities = np.full((num_tags, num_words, num_tags), -np.inf)\n",
    "    \n",
    "    next_word_indices = word_indices[1:]\n",
    "    \n",
    "    # Compute the log probabilities using broadcasting\n",
    "    # This replaces the nested loops with vectorized operations\n",
    "        # Compute the full tensor of log probabilities for this word position\n",
    "    return (\n",
    "        log_forward[:, :-1, np.newaxis] +  # first tag dimension\n",
    "        log_backward[:, 1:].T[np.newaxis, :] +  # second tag dimension\n",
    "        transition[:, np.newaxis, :] +  # transition probabilities\n",
    "        emission[:, next_word_indices].T[np.newaxis, :] -  # emission for next word\n",
    "        log_forward_val\n",
    "    )\n",
    "    \n",
    "    return si_probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['Noun', 'Verb', 'Adjective']\n",
      "Word Indices: [0, 1, 2, 3]\n",
      "Word Sequence: ['the', 'quick', 'brown', 'fox']\n",
      "Forward Matrix:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "Backward Matrix:\n",
      " [[120 110 100  90]\n",
      " [ 80  70  60  50]\n",
      " [ 40  30  20  10]]\n",
      "Forward Value: 10\n",
      "Transition Matrix:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Emission Matrix:\n",
      " [[ 11  22  33  44]\n",
      " [ 88  77  66  55]\n",
      " [ 99 111 222 333]]\n",
      "SI Probabilities from log_si_probs:\n",
      " [[[124. 140. 135.]\n",
      "  [126. 120. 237.]\n",
      "  [128. 100. 339.]]\n",
      "\n",
      " [[131. 147. 142.]\n",
      "  [133. 127. 244.]\n",
      "  [135. 107. 346.]]\n",
      "\n",
      " [[138. 154. 149.]\n",
      "  [140. 134. 251.]\n",
      "  [142. 114. 353.]]]\n",
      "SI Probabilities from log_si_probs_vec:\n",
      " [[[124 140 135]\n",
      "  [126 120 237]\n",
      "  [128 100 339]]\n",
      "\n",
      " [[131 147 142]\n",
      "  [133 127 244]\n",
      "  [135 107 346]]\n",
      "\n",
      " [[138 154 149]\n",
      "  [140 134 251]\n",
      "  [142 114 353]]]\n",
      "Are the outputs equivalent? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the parameters\n",
    "tags = ['Noun', 'Verb', 'Adjective']\n",
    "word_sequence = ['the', 'quick', 'brown', 'fox']\n",
    "word_indices = [0, 1, 2, 3]  # corresponding indices in a hypothetical vocabulary\n",
    "log_forward = np.array([\n",
    "    [1, 2, 3, 4],  # Values for Noun\n",
    "    [5, 6, 7, 8],  # Values for Verb\n",
    "    [9, 10, 11, 12]  # Values for Adjective\n",
    "])\n",
    "log_backward = np.array([\n",
    "    [120, 110, 100, 90],  # Values for Noun\n",
    "    [80, 70, 60, 50],  # Values for Verb\n",
    "    [40, 30, 20, 10]  # Values for Adjective\n",
    "])\n",
    "log_forward_val = 10\n",
    "\n",
    "# Define transition matrix with distinct values\n",
    "transition = np.array([\n",
    "    [1, 2, 3],  # From Noun to Noun, Verb, Adjective\n",
    "    [4, 5, 6],  # From Verb to Noun, Verb, Adjective\n",
    "    [7, 8, 9]   # From Adjective to Noun, Verb, Adjective\n",
    "])\n",
    "# Define emission matrix with distinct values\n",
    "vocab_size = max(word_indices) + 1\n",
    "emission = np.array([\n",
    "    [11, 22, 33, 44],  # Emissions for Noun\n",
    "    [88, 77, 66, 55],  # Emissions for Verb\n",
    "    [99, 111, 222, 333]   # Emissions for Adjective\n",
    "])\n",
    "\n",
    "# Print input values\n",
    "print(\"Tags:\", tags)\n",
    "print(\"Word Indices:\", word_indices)\n",
    "print(\"Word Sequence:\", word_sequence)\n",
    "print(\"Forward Matrix:\\n\", log_forward)\n",
    "print(\"Backward Matrix:\\n\", log_backward)\n",
    "print(\"Forward Value:\", log_forward_val)\n",
    "print(\"Transition Matrix:\\n\", transition)\n",
    "print(\"Emission Matrix:\\n\", emission)\n",
    "\n",
    "# Use the original function\n",
    "si_probabilities = log_si_probs(tags, word_indices, log_forward, log_backward, log_forward_val, transition, emission)\n",
    "print(\"SI Probabilities from log_si_probs:\\n\", si_probabilities)\n",
    "\n",
    "# Use the vectorized function\n",
    "si_probabilities_vec = log_si_probs_vec(tags, word_indices, log_forward, log_backward, log_forward_val, transition, emission)\n",
    "print(\"SI Probabilities from log_si_probs_vec:\\n\", si_probabilities_vec)\n",
    "\n",
    "# Check if the outputs are equivalent\n",
    "print(\"Are the outputs equivalent?\", np.allclose(si_probabilities, si_probabilities_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_forward shape: (3, 16)\n",
      "log_backward shape: (3, 16)\n",
      "log_transition shape: (3, 3)\n",
      "log_emission shape: (3, 16)\n",
      "log_forward_expanded shape: (3, 15, 1)\n",
      "log_backward_expanded shape: (1, 15, 3)\n",
      "log_transition_expanded shape: (3, 1, 3)\n",
      "log_emission_expanded shape: (1, 15, 3)\n",
      "si_probs shape: (3, 15, 3)\n",
      "[[[1.51029713 1.38980638 1.52449204]\n",
      "  [2.29366963 1.08334145 2.11525303]\n",
      "  [1.56355873 2.0600615  1.57484353]\n",
      "  [0.89286649 1.14084519 1.61973685]\n",
      "  [2.14841464 2.60954947 2.17309765]\n",
      "  [1.60233251 2.08336332 1.60663009]\n",
      "  [1.7372244  2.71781189 1.16597556]\n",
      "  [2.12092602 1.81901328 1.62118963]\n",
      "  [1.74210682 1.38239048 2.04785704]\n",
      "  [2.08971187 1.98999997 1.93478329]\n",
      "  [2.31690699 2.4274563  1.91590583]\n",
      "  [1.45032894 2.48323461 2.02261856]\n",
      "  [0.6601869  2.02478702 1.68270745]\n",
      "  [1.74223453 2.78878149 1.70630344]\n",
      "  [2.69219542 2.88730913 2.36281998]]\n",
      "\n",
      " [[2.03880481 1.89595267 2.57276611]\n",
      "  [2.02446347 0.79177391 2.36581327]\n",
      "  [1.29469551 1.76883689 1.82574669]\n",
      "  [1.27226576 1.49788306 2.5189025 ]\n",
      "  [1.56740831 2.00618175 2.11185771]\n",
      "  [1.22841934 1.68708877 1.75248332]\n",
      "  [1.51983644 2.47806254 1.46835398]\n",
      "  [2.00662944 1.68235531 2.02665944]\n",
      "  [1.48533409 1.10325636 2.31085071]\n",
      "  [2.39779408 2.27572079 2.76263189]\n",
      "  [2.11988379 2.20807172 2.23864902]\n",
      "  [1.85341261 2.86395689 2.94546862]\n",
      "  [0.52449644 1.86673517 2.06678338]\n",
      "  [1.8518866  2.87607218 2.33572191]\n",
      "  [2.23791427 2.41066659 2.42830521]]\n",
      "\n",
      " [[1.93805651 1.52150045 1.8743376 ]\n",
      "  [2.63382991 1.12743643 2.3774995 ]\n",
      "  [2.02862917 2.22906664 1.96200015]\n",
      "  [1.39239521 1.3443086  2.04135175]\n",
      "  [2.15706489 2.32213441 2.10383408]\n",
      "  [1.77103808 1.95600358 1.69742184]\n",
      "  [2.20600453 2.8905267  1.55684186]\n",
      "  [2.31904176 1.72106372 1.74139156]\n",
      "  [2.45403014 1.79824849 2.68186655]\n",
      "  [2.74697375 2.35119654 2.51413135]\n",
      "  [2.38634278 2.20082679 1.9074278 ]\n",
      "  [1.96873176 2.70557213 2.46310757]\n",
      "  [1.05611212 2.12464693 2.00071885]\n",
      "  [2.55770038 3.30818203 2.44385547]\n",
      "  [2.54484033 2.44388873 2.13755107]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_tags = 3\n",
    "num_words = 16\n",
    "word_indices = np.array([i for i in range(16)])  # Example word indices\n",
    "log_forward = np.random.rand(num_tags, num_words)  # Shape: (num_tags, num_words)\n",
    "log_backward = np.random.rand(num_tags, num_words)  # Shape: (num_tags, num_words)\n",
    "log_transition = np.random.rand(num_tags, num_tags)  # Shape: (num_tags, num_tags)\n",
    "log_emission = np.random.rand(num_tags, num_words)  # Shape: (num_tags, num_words)\n",
    "log_forward_val = np.random.rand()  # Scalar for simplicity\n",
    "\n",
    "# Prepare for vectorized computation\n",
    "next_word_indices = word_indices[1:]  # Shape: (num_words - 1,)\n",
    "\n",
    "# Expand dimensions to align shapes for broadcasting\n",
    "log_forward_expanded = log_forward[:, :-1, np.newaxis]\n",
    "log_backward_expanded = log_backward[:, 1:].T[np.newaxis, :]\n",
    "log_transition_expanded = log_transition[:, np.newaxis, :]\n",
    "log_emission_expanded = log_emission[:, next_word_indices].T[np.newaxis, :]\n",
    "\n",
    "# Print results to verify shapes\n",
    "print(\"log_forward shape:\", log_forward.shape)\n",
    "print(\"log_backward shape:\", log_backward.shape)\n",
    "print(\"log_transition shape:\", log_transition.shape)\n",
    "print(\"log_emission shape:\", log_emission.shape)\n",
    "print(\"log_forward_expanded shape:\", log_forward_expanded.shape)\n",
    "print(\"log_backward_expanded shape:\", log_backward_expanded.shape)\n",
    "print(\"log_transition_expanded shape:\", log_transition_expanded.shape)\n",
    "print(\"log_emission_expanded shape:\", log_emission_expanded.shape)\n",
    "\n",
    "# Fully vectorized computation\n",
    "si_probs = (\n",
    "    log_forward_expanded +\n",
    "    log_backward_expanded +\n",
    "    log_transition_expanded +\n",
    "    log_emission_expanded -\n",
    "    log_forward_val\n",
    ")  # Final shape: (num_tags, num_words-1, num_tags)\n",
    "print(\"si_probs shape:\", si_probs.shape)\n",
    "\n",
    "\n",
    "print(si_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
